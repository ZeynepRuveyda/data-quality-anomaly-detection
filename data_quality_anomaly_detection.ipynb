{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Data Quality & Anomaly Detection in Mobility Systems\n",
        "## PhD Research Project - COSYS/GRETTIA, Universit√© Gustave Eiffel\n",
        "\n",
        "### üéØ Research Objectives\n",
        "- **Data Quality Assessment**: Missing values, extreme values, data consistency\n",
        "- **Anomaly Detection**: Traditional and ML-based methods\n",
        "- **AI/ML Implementation**: Isolation Forest, Random Forest, DBSCAN\n",
        "- **Auto-parameter Tuning**: Hyperparameter optimization\n",
        "- **Mobility Applications**: Transportation systems analysis\n",
        "\n",
        "---\n",
        "*This notebook demonstrates comprehensive data quality analysis and anomaly detection techniques applied to mobility data.*\n",
        "*Bu notebook, mobilite verilerine uygulanan kapsamlƒ± veri kalitesi analizi ve anomali tespit tekniklerini g√∂sterir.*\n",
        "*Ce notebook d√©montre des techniques compl√®tes d'analyse de la qualit√© des donn√©es et de d√©tection d'anomalies appliqu√©es aux donn√©es de mobilit√©.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully\n",
            "‚úÖ K√ºt√ºphaneler ba≈üarƒ±yla import edildi\n",
            "‚úÖ Biblioth√®ques import√©es avec succ√®s\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*\n",
        "# H√úCRE 1: K√úT√úPHANELER VE ƒ∞MPORTLAR\n",
        "# CELLULE 1: BIBLIOTH√àQUES ET IMPORTS\n",
        "# CELL 1: LIBRARIES AND IMPORTS\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import stats\n",
        "from scipy.stats import randint, uniform\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(\"‚úÖ K√ºt√ºphaneler ba≈üarƒ±yla import edildi\")\n",
        "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 2: VERƒ∞ Y√úKLEME VE TEMEL ƒ∞STATƒ∞STƒ∞KLER\n",
        "# CELLULE 2: CHARGEMENT ET STATISTIQUES DE BASE\n",
        "# CELL 2: DATA LOADING AND BASIC STATISTICS\n",
        "\n",
        "# Load the CSV data\n",
        "data = pd.read_csv('anomalies_detected.csv')\n",
        "\n",
        "print(\"üìä Dataset Overview:\")\n",
        "print(f\"Shape: {data.shape}\")\n",
        "print(f\"Columns: {list(data.columns)}\")\n",
        "print(\"\n",
        "ÔøΩÔøΩ Basic Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "print(\"\n",
        "‚ùå Missing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(\"\n",
        "üìã Data Types:\")\n",
        "print(data.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 3: ANOMALƒ∞ TESPƒ∞Tƒ∞ ANALƒ∞Zƒ∞\n",
        "# CELLULE 3: ANALYSE DE D√âTECTION D'ANOMALIES\n",
        "# CELL 3: ANOMALY DETECTION ANALYSIS\n",
        "\n",
        "# Analyze anomaly detection results\n",
        "print(\"üéØ ANOMALY DETECTION ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Count anomalies for each method\n",
        "zscore_count = data['anomaly_zscore'].sum()\n",
        "iqr_count = data['anomaly_iqr'].sum()\n",
        "isolation_count = data['anomaly_isolation'].sum()\n",
        "\n",
        "print(f\"Z-Score Anomalies: {zscore_count}\")\n",
        "print(f\"IQR Anomalies: {iqr_count}\")\n",
        "print(f\"Isolation Forest Anomalies: {isolation_count}\")\n",
        "\n",
        "# Find common anomalies\n",
        "common_anomalies = data[data['anomaly_zscore'] & data['anomaly_iqr'] & data['anomaly_isolation']]\n",
        "print(f\"\n",
        "üîç Common Anomalies (All Methods): {len(common_anomalies)}\")\n",
        "\n",
        "# Show some examples\n",
        "if len(common_anomalies) > 0:\n",
        "    print(\"\n",
        "üîç Examples of Common Anomalies:\")\n",
        "    print(common_anomalies[['trip_duration_min', 'speed_kmh', 'latitude', 'longitude']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 4: VERƒ∞ KALƒ∞TESƒ∞ ANALƒ∞Zƒ∞\n",
        "# CELLULE 4: ANALYSE DE LA QUALIT√â DES DONN√âES\n",
        "# CELL 4: DATA QUALITY ANALYSIS\n",
        "\n",
        "# Data Quality Analysis\n",
        "print(\"üîç DATA QUALITY ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check for missing values\n",
        "missing_data = data.isnull().sum()\n",
        "missing_percentage = (missing_data / len(data)) * 100\n",
        "\n",
        "print(\"üìä Missing Data Analysis:\")\n",
        "for col, count, percentage in zip(missing_data.index, missing_data.values, missing_percentage.values):\n",
        "    if count > 0:\n",
        "        print(f\"{col}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "# Check for extreme values\n",
        "print(\"\n",
        "üìà Extreme Value Analysis:\")\n",
        "for col in ['trip_duration_min', 'speed_kmh']:\n",
        "    if col in data.columns:\n",
        "        q1 = data[col].quantile(0.25)\n",
        "        q3 = data[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        \n",
        "        extreme_low = (data[col] < lower_bound).sum()\n",
        "        extreme_high = (data[col] > upper_bound).sum()\n",
        "        \n",
        "        print(f\"{col}:\")\n",
        "        print(f\"  Extreme Low: {extreme_low}\")\n",
        "        print(f\"  Extreme High: {extreme_high}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 5: GELƒ∞≈ûMƒ∞≈û ANOMALƒ∞ TESPƒ∞Tƒ∞\n",
        "# CELLULE 5: D√âTECTION D'ANOMALIES AVANC√âE\n",
        "# CELL 5: ADVANCED ANOMALY DETECTION\n",
        "\n",
        "# Advanced Anomaly Detection Methods\n",
        "print(\"üîç ADVANCED ANOMALY DETECTION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare data for advanced methods\n",
        "numeric_cols = ['trip_duration_min', 'speed_kmh']\n",
        "data_clean = data[numeric_cols].dropna()\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_clean)\n",
        "\n",
        "# 1. DBSCAN Clustering\n",
        "print(\"üîç 1. DBSCAN Clustering Analysis\")\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(data_scaled)\n",
        "dbscan_anomalies = (dbscan_labels == -1).sum()\n",
        "print(f\"DBSCAN Anomalies: {dbscan_anomalies}\")\n",
        "\n",
        "# 2. PCA-based Anomaly Detection\n",
        "print(\"\n",
        "üîç 2. PCA-based Anomaly Detection\")\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "\n",
        "# Calculate reconstruction error\n",
        "data_reconstructed = pca.inverse_transform(data_pca)\n",
        "reconstruction_error = np.mean((data_scaled - data_reconstructed) ** 2, axis=1)\n",
        "pca_threshold = np.percentile(reconstruction_error, 95)\n",
        "pca_anomalies = (reconstruction_error > pca_threshold).sum()\n",
        "print(f\"PCA Anomalies: {pca_anomalies}\")\n",
        "\n",
        "# 3. Ensemble Method\n",
        "print(\"\n",
        "üîç 3. Ensemble Method\")\n",
        "ensemble_scores = np.zeros(len(data_clean))\n",
        "\n",
        "# Z-score method\n",
        "z_scores = np.abs(stats.zscore(data_clean))\n",
        "ensemble_scores += (z_scores > 3).any(axis=1)\n",
        "\n",
        "# IQR method\n",
        "Q1 = data_clean.quantile(0.25)\n",
        "Q3 = data_clean.quantile(0.25)\n",
        "IQR = Q3 - Q1\n",
        "iqr_outliers = ((data_clean < (Q1 - 1.5 * IQR)) | (data_clean > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "ensemble_scores += iqr_outliers\n",
        "\n",
        "# Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "iso_pred = iso_forest.fit_predict(data_clean)\n",
        "ensemble_scores += (iso_pred == -1)\n",
        "\n",
        "# Final ensemble decision\n",
        "ensemble_anomalies = (ensemble_scores >= 2).sum()\n",
        "print(f\"Ensemble Anomalies: {ensemble_anomalies}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 6: Hƒ∞PERPARAMETRE OPTƒ∞Mƒ∞ZASYONU\n",
        "# CELLULE 6: OPTIMISATION DES HYPERPARAM√àTRES\n",
        "# CELL 6: HYPERPARAMETER OPTIMIZATION\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "print(\"‚öôÔ∏è HYPERPARAMETER OPTIMIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare data for ML models\n",
        "X = data_clean\n",
        "y = (ensemble_scores >= 2).astype(int)  # Use ensemble as ground truth\n",
        "\n",
        "# Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Isolation Forest Optimization\n",
        "print(\"üîç 1. Isolation Forest Optimization\")\n",
        "iso_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'contamination': [0.05, 0.1, 0.15],\n",
        "    'max_samples': ['auto', 100, 200]\n",
        "}\n",
        "\n",
        "iso_grid = GridSearchCV(IsolationForest(random_state=42), iso_param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "iso_grid.fit(X_train)\n",
        "\n",
        "print(f\"Best Isolation Forest Parameters: {iso_grid.best_params_}\")\n",
        "print(f\"Best Score: {iso_grid.best_score_:.4f}\")\n",
        "\n",
        "# 2. Random Forest for Anomaly Classification\n",
        "print(\"\n",
        "üîç 2. Random Forest Optimization\")\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3, scoring='f1')\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Random Forest Parameters: {rf_grid.best_params_}\")\n",
        "print(f\"Best F1 Score: {rf_grid.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 7: PERFORMANS DEƒûERLENDƒ∞RMESƒ∞\n",
        "# CELLULE 7: √âVALUATION DES PERFORMANCES\n",
        "# CELL 7: PERFORMANCE EVALUATION\n",
        "\n",
        "# Performance Evaluation\n",
        "print(\"üìä PERFORMANCE EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Evaluate best models\n",
        "best_iso = iso_grid.best_estimator_\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "# Isolation Forest predictions\n",
        "iso_pred = best_iso.predict(X_test)\n",
        "iso_pred_binary = (iso_pred == -1).astype(int)\n",
        "\n",
        "# Random Forest predictions\n",
        "rf_pred = best_rf.predict(X_test)\n",
        "rf_prob = best_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "print(\"üéØ Isolation Forest Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, iso_pred_binary):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, iso_pred_binary):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, iso_pred_binary):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, iso_pred_binary):.4f}\")\n",
        "\n",
        "print(\"\n",
        "üéØ Random Forest Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, rf_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, rf_pred):.4f}\")\n",
        "print(f\"F1-Score: {accuracy_score(y_test, rf_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, rf_prob):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√úCRE 8: G√ñRSELLE≈ûTƒ∞RME VE SONU√áLAR\n",
        "# CELLULE 8: VISUALISATION ET R√âSULTATS\n",
        "# CELL 8: VISUALIZATION AND RESULTS\n",
        "\n",
        "# Visualization and Results\n",
        "print(\"üìä VISUALIZATION AND RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Trip Duration Distribution\n",
        "axes[0, 0].hist(data['trip_duration_min'].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('Trip Duration Distribution\n",
        "Daƒüƒ±lƒ±mƒ± / Distribution')\n",
        "axes[0, 0].set_xlabel('Duration (min)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# 2. Speed Distribution\n",
        "axes[0, 1].hist(data['speed_kmh'].dropna(), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[0, 1].set_title('Speed Distribution\n",
        "Daƒüƒ±lƒ±mƒ± / Distribution')\n",
        "print(\"\n",
        "üìà Speed Distribution:\")\n",
        "print(f\"Speed range: {data['speed_kmh'].min():.2f} - {data['speed_kmh'].max():.2f} km/h\")\n",
        "print(f\"Hƒ±z aralƒ±ƒüƒ±: {data['speed_kmh'].min():.2f} - {data['speed_kmh'].max():.2f} km/saat\")\n",
        "print(f\"Plage de vitesse: {data['speed_kmh'].min():.2f} - {data['speed_kmh'].max():.2f} km/h\")\n",
        "\n",
        "print(\"‚úÖ Data quality analysis completed!\")\n",
        "print(\"‚úÖ Veri kalitesi analizi tamamlandƒ±!\")\n",
        "print(\"‚úÖ Analyse de la qualit√© des donn√©es termin√©e!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
